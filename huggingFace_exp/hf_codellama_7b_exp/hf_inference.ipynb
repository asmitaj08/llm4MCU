{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1954c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "chroma_db_path = \"hf_codellama_7b_exp/chroma_dbs/nRF52840_db\"\n",
    "pickle_path = \"hf_codellama_7b_exp/pickle_files/nRF52840_summarized.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0045558-b00c-4ca9-9d6c-dd8495a522c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 13:27:03.236908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746710823.254702   93659 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746710823.260132   93659 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746710823.273607   93659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746710823.273619   93659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746710823.273621   93659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746710823.273622   93659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-08 13:27:03.278800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "import base64\n",
    "import os\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from io import BytesIO\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "import uuid\n",
    "from langchain_core.runnables import RunnableLambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674e2bbb-972d-4935-b740-b9659e914b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "base_model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# embedd_model = \"sentence-transformers/all-MiniLM-L6-v2\" #will have to experiment with embeddig models\n",
    "# embedd_model = \"intfloat/e5-large-v2\"\n",
    "embedd_model = \"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee646082-38d4-4bd8-b2d7-d2327051d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=embedd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba5d67d-9390-4bf4-b601-7ef20f8898df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7e262b475943d099fbb8587d1025bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Create Hugging Face pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=model.config.eos_token_id, #avoiding warning Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
    "    max_new_tokens=768,\n",
    "    # temperature=0.0, # no need as do_sample=False\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Wrap it in LangChain-compatible interface\n",
    "model_pipe = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63ba4a4c-58d9-49c2-87ba-619d23530546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroma_db(local_directory=chroma_db_path):\n",
    "    return Chroma(persist_directory=local_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b77592d-5f3c-4263-bb90-b0b903c5900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "    \n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images) #changing this as codellama doesn't deal directly with images\n",
    "        add_documents(retriever, image_summaries, image_summaries)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8435c003-a90e-48f8-ad25-912c9b3199fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# def split_image_text_types(docs):\n",
    "#     \"\"\"\n",
    "#     Split base64-encoded images and texts\n",
    "#     \"\"\"\n",
    "#     b64_images = []\n",
    "#     texts = []\n",
    "#     for doc in docs:\n",
    "#         # Check if the document is of type Document and extract page_content if so\n",
    "#         if isinstance(doc, Document):\n",
    "#             doc = doc.page_content\n",
    "#         if looks_like_base64(doc) and is_image_data(doc):\n",
    "#             doc = resize_base64_image(doc, size=(1300, 600))\n",
    "#             b64_images.append(doc)\n",
    "#         else:\n",
    "#             texts.append(doc)\n",
    "#     if len(b64_images) > 0:\n",
    "#         return {\"images\": b64_images[:1], \"texts\": []}\n",
    "#     return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        if isinstance(doc, Document):\n",
    "            content = doc.page_content\n",
    "        else:\n",
    "            content = doc\n",
    "\n",
    "        # If this document is a base64 image (raw), skip it\n",
    "        if looks_like_base64(content) and is_image_data(content): #we don'rt need it in this case as we r not dealing with images during inferences\n",
    "            print(\"\\n**********found base64\")\n",
    "            continue  # raw image, not usable here\n",
    "        else:\n",
    "            texts.append(content)\n",
    "\n",
    "    return {\"images\": [], \"texts\": texts}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "474ad6b7-5cc0-42cc-a4a8-9131d18af044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def img_prompt_func(data_dict):\n",
    "#     \"\"\"\n",
    "#     Join the context into a single string\n",
    "#     \"\"\"\n",
    "#     messages = []\n",
    "\n",
    "#     # Adding the text for analysis\n",
    "#     text_message = {\n",
    "#         \"type\": \"text\",\n",
    "#         \"text\": (\n",
    "#             \"You are an AI scientist tasking with providing factual answers from a datasheet of a System-on-Chip (SoC) \\n\"\n",
    "#             \"Use this information to provide answers related to the user question. \\n\"\n",
    "#             f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "#         ),\n",
    "#     }\n",
    "#     messages.append(text_message)\n",
    "#     # Adding image(s) to the messages if present\n",
    "#     if data_dict[\"context\"][\"images\"]:\n",
    "#         for image in data_dict[\"context\"][\"images\"]:\n",
    "#             image_message = {\n",
    "#                 \"type\": \"image_url\",\n",
    "#                 \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "#             }\n",
    "#             messages.append(image_message)\n",
    "#     return [HumanMessage(content=messages)]\n",
    "\n",
    "# def text_only_prompt_func(data_dict):\n",
    "#     context = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "\n",
    "#     return f\"\"\"\n",
    "# You are an AI assistant helping analyze System-on-Chip (SoC) datasheets.\n",
    "# Answer the following question using the provided datasheet context only.\n",
    "\n",
    "# Question:\n",
    "# {data_dict[\"question\"]}\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "# \"\"\"\n",
    "\n",
    "# def text_only_prompt_func(data_dict):\n",
    "#     \"\"\"\n",
    "#     Formats the retrieved context into a plain prompt string for HuggingFacePipeline\n",
    "#     \"\"\"\n",
    "#     # prompt = (\n",
    "#     #     \"You are an AI assistant helping interpret information from a datasheet of a System-on-Chip (SoC).\\n\"\n",
    "#     #     \"Answer the user's question using the relevant text summaries provided below.\\n\\n\"\n",
    "#     #     f\"User Question:\\n{data_dict['question']}\\n\\n\"\n",
    "#     # )\n",
    "#     prompt = (\n",
    "#         \"You are an expert assistant for Microcontroller datasheets.\\n\"\n",
    "#         \"Answer the user question concisely based  on the context provided.\\n\"\n",
    "#         \"Do not repeat the context. Do not explain your reasoning. Just give the final answer.\\n\\n\"\n",
    "#         f\"User question: {data_dict['question']}\\n\\n\"\n",
    "#     )\n",
    "\n",
    "#     if data_dict[\"context\"][\"texts\"]:\n",
    "#         prompt += \"Relevant Context:\\n\" + \"\\n\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "\n",
    "#     return prompt\n",
    "\n",
    "def text_only_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Formats a CodeLLaMA-compatible prompt with summaries and question.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an expert on microcontrollers and can provide detailed information about their peripherals, registers, and fields.\\n\"\n",
    "        \"Answer the user question concisely based  on the context provided.\\n\"\n",
    "        \"ONLY output the direct answer as a word, number, address, or short phrase.\\n\"\n",
    "        \"Do NOT repeat the question or context. Do NOT give explanations or full sentences.\\n\\n\"\n",
    "        # f\"User question: {data_dict['question']}\\n\\n\"\n",
    "    )\n",
    "    if data_dict[\"context\"][\"texts\"]:\n",
    "        prompt += \"Relevant Context:\\n\" + \"\\n\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "\n",
    "    instruction_block = (\n",
    "        f\"Context:\\n{prompt}\\n\\n\"\n",
    "        f\"Question: {data_dict['question']}\"\n",
    "    )\n",
    "    # context=\"\"\n",
    "    # if data_dict[\"context\"][\"texts\"]:\n",
    "    #     context = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    # formatted_prompt = (\n",
    "    #     f\"### Instruction:\\n{prompt}\\n\"\n",
    "    #     f\"Context:\\n{context}\\n\\n\"\n",
    "    #     f\"Question: {data_dict['question']}\\n\\n\"\n",
    "    #     f\"### Response:\\n\"\n",
    "    # )\n",
    "\n",
    "    # Final CodeLLaMA prompt format\n",
    "    return f\"### Instruction:\\n{instruction_block}\\n\\n### Response:\\n\"\n",
    "    # return formatted_prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(text_only_prompt_func)\n",
    "        | model_pipe  # MM_LLM\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3027535f-3e13-40b8-8043-8d017f3dd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rag(chroma_path, pickle_path):\n",
    "    # if os.path.exists(db_path) and os.path.exists(pickle_path):\n",
    "    print(\"Loading existing Chroma database...\")\n",
    "    vectorstore = load_chroma_db(chroma_path)\n",
    "    \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "\n",
    "    # Access the variables\n",
    "    texts = loaded_data['texts']\n",
    "    tables = loaded_data['tables']\n",
    "    text_summaries = loaded_data['text_summaries']\n",
    "    table_summaries = loaded_data['table_summaries']\n",
    "    img_base64_list = loaded_data['img_base64_list']\n",
    "    image_summaries = loaded_data['image_summaries']\n",
    "\n",
    "    retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "        vectorstore,\n",
    "        text_summaries,\n",
    "        texts,\n",
    "        table_summaries,\n",
    "        tables,\n",
    "        image_summaries,\n",
    "        img_base64_list,\n",
    "    )\n",
    "    chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)\n",
    "    return chain_multimodal_rag,retriever_multi_vector_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "225b8c8e-8207-4556-8ce9-645cd8dedd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ask_bot(chain_multimodal_rag, query):\n",
    "#     # docs = retriever_multi_vector_img.get_relevant_documents(query, limit=10)\n",
    "#     # print(split_image_text_types(docs))\n",
    "#     return chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f1720-0ca9-4845-857f-d1cd4a933c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3e1d2a8-cd73-44da-aaa4-1aabbc9547e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing Chroma database...\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline, retriever_multi_vector_img = init_rag(chroma_db_path, pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203b579-0051-4adc-974f-a5a05098580b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47d918-225b-4af2-9321-fd9e45dbb522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00047dce-621d-4450-b18a-d34985e5703d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(retriever_multi_vector_img.search_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d34f2307-0186-40ba-85b5-3c0eac12808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def ask_bot(chain_multimodal_rag, query):\n",
    "    raw_output = chain_multimodal_rag.invoke(query)\n",
    "    # print(f\"Raw output : {raw_output}\\n\")\n",
    "    # print(\"*********************\")\n",
    "\n",
    "    # Extract after prompt marker\n",
    "    if \"### Response:\" in raw_output:\n",
    "        answer = raw_output.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        answer = raw_output.strip()\n",
    "\n",
    "    # Normalize: clean each line\n",
    "    lines = [\n",
    "        re.sub(r\"[.,:;!?]+$\", \"\", line.strip())  # Remove trailing punctuation\n",
    "        for line in answer.splitlines()\n",
    "        if line.strip()\n",
    "    ]\n",
    "\n",
    "    return \", \".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ae6fa-bca2-478e-9a83-4cfc5c633023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bot_manual(retriever_multi_vector_img, question: str) -> str:\n",
    "    # Step 1: Retrieve relevant summaries (text + table + image) via RAG\n",
    "    docs = retriever_multi_vector_img.get_relevant_documents(question, k=8)\n",
    "    summaries = [\n",
    "    doc if isinstance(doc, str) else doc.page_content\n",
    "    for doc in retriever_multi_vector_img.get_relevant_documents(question, k=8)\n",
    "   ]\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an expert on microcontrollers and can provide detailed information about their peripherals, registers, and fields.\\n\"\n",
    "        \"Answer the user question concisely based  on the context provided.\\n\"\n",
    "        \"ONLY output the direct answer as a word, number, address, or short phrase.\\n\"\n",
    "        \"Do NOT repeat the question or context. Do NOT give explanations or full sentences.\\n\\n\"\n",
    "        # f\"User question: {data_dict['question']}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Build context prompt\n",
    "    context = \"\\n\".join(summaries)\n",
    "    formatted_prompt = (\n",
    "        f\"### Instruction:\\n{prompt}\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Tokenize and generate\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=768,\n",
    "            #temperature=0.0,  # deterministic\n",
    "            top_p=0.95,\n",
    "            do_sample=False,\n",
    "            pad_token_id=model.config.eos_token_id\n",
    "        )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Step 4: Extract answer cleanly\n",
    "    answer = generated_text.replace(formatted_prompt, \"\").strip()\n",
    "    return answer.split(\"\\n\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7f398-f61b-4d52-8df8-6abfebcd0480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b9cb116-445d-4d6e-95ac-d18fba306a10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ask_bot_new1(prompt_chain, question: str) -> str:\n",
    "    # Step 1: Generate the full CodeLLaMA-style prompt using the chain\n",
    "    formatted_prompt = prompt_chain.invoke(question)\n",
    "\n",
    "    # Step 2: Run your manual inference\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=768,\n",
    "            # temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            do_sample=False,\n",
    "            pad_token_id=model.config.eos_token_id #avoiding warning Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
    "\n",
    "        )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Step 3: Extract only the answer\n",
    "    return generated_text.replace(formatted_prompt, \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "185354ba-5534-49e8-ba73-581a105c3edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output : ### Instruction:\n",
      "Context:\n",
      "You are an expert on microcontrollers and can provide detailed information about their peripherals, registers, and fields.\n",
      "Answer the user question concisely based  on the context provided.\n",
      "ONLY output the direct answer as a word, number, address, or short phrase.\n",
      "Do NOT repeat the question or context. Do NOT give explanations or full sentences.\n",
      "\n",
      "\n",
      "\n",
      "Question: List UART regsiters for nrf52840 Microcontroller\n",
      "\n",
      "### Response:\n",
      "\n",
      "UART0_CONFIG\n",
      "UART0_BAUD\n",
      "UART0_RXD\n",
      "UART0_TXD\n",
      "UART0_CTS\n",
      "UART0_RTS\n",
      "UART0_STATUS\n",
      "UART0_ERRORSRC\n",
      "UART0_ENABLE\n",
      "UART0_MATCH\n",
      "UART0_MASK\n",
      "UART0_PSELRXD\n",
      "UART0_PSELTXD\n",
      "UART0_PSELRTS\n",
      "UART0_PSELCTS\n",
      "UART0_BAUDRATE\n",
      "UART0_RXD_PTR\n",
      "UART0_RXD_MAXCNT\n",
      "UART0_RXD_AMOUNT\n",
      "UART0_RXD_LIST\n",
      "UART0_RXD_LIST_MAXCNT\n",
      "UART0_RXD_LIST_AMOUNT\n",
      "UART0_TXD_PTR\n",
      "UART0_TXD_MAXCNT\n",
      "UART0_TXD_AMOUNT\n",
      "UART0_TXD_LIST\n",
      "UART0_TXD_LIST_MAXCNT\n",
      "UART0_TXD_LIST_AMOUNT\n",
      "UART0_ERRORSRC_REG\n",
      "UART0_ENABLE_REG\n",
      "UART0_MATCH_REG\n",
      "UART0_MASK_REG\n",
      "UART0_PSELRXD_REG\n",
      "UART0_PSELTXD_REG\n",
      "UART0_PSELRTS_REG\n",
      "UART0_PSELCTS_REG\n",
      "UART0_BAUDRATE_REG\n",
      "UART0_RXD_PTR_REG\n",
      "UART0_RXD_MAXCNT_REG\n",
      "UART0_RXD_AMOUNT_REG\n",
      "UART0_RXD_LIST_REG\n",
      "UART0_RXD_LIST_MAXCNT_REG\n",
      "UART0_RXD_LIST_AMOUNT_REG\n",
      "UART0_TXD_PTR_REG\n",
      "UART0_TXD_MAXCNT_REG\n",
      "UART0_TXD_AMOUNT_REG\n",
      "UART0_TXD_LIST_REG\n",
      "UART0_TXD_LIST_MAXCNT_REG\n",
      "UART0_TXD_LIST_AMOUNT_REG\n",
      "UART0_STATUS_REG\n",
      "UART0_ERRORSRC_MASK\n",
      "UART0_ENABLE_MASK\n",
      "UART0_MATCH_MASK\n",
      "UART0_MASK_MASK\n",
      "UART0_PSELRXD_MASK\n",
      "UART0_PSELTXD_MASK\n",
      "UART0_PSELRTS_MASK\n",
      "UART0_PSELCTS_MASK\n",
      "UART0_BAUDRATE_MASK\n",
      "UART0_RXD_PTR_MASK\n",
      "UART0_RXD_MAXCNT_MASK\n",
      "UART0_RXD_AMOUNT_MASK\n",
      "UART0_RXD_LIST_MASK\n",
      "UART0_RXD_LIST_MAXCNT_MASK\n",
      "UART0_RXD_LIST_AMOUNT_MASK\n",
      "UART0_TXD_PTR_MASK\n",
      "UART0_TXD_MAXCNT_MASK\n",
      "UART0_TXD_AMOUNT_MASK\n",
      "\n",
      "*********************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'UART0_CONFIG, UART0_BAUD, UART0_RXD, UART0_TXD, UART0_CTS, UART0_RTS, UART0_STATUS, UART0_ERRORSRC, UART0_ENABLE, UART0_MATCH, UART0_MASK, UART0_PSELRXD, UART0_PSELTXD, UART0_PSELRTS, UART0_PSELCTS, UART0_BAUDRATE, UART0_RXD_PTR, UART0_RXD_MAXCNT, UART0_RXD_AMOUNT, UART0_RXD_LIST, UART0_RXD_LIST_MAXCNT, UART0_RXD_LIST_AMOUNT, UART0_TXD_PTR, UART0_TXD_MAXCNT, UART0_TXD_AMOUNT, UART0_TXD_LIST, UART0_TXD_LIST_MAXCNT, UART0_TXD_LIST_AMOUNT, UART0_ERRORSRC_REG, UART0_ENABLE_REG, UART0_MATCH_REG, UART0_MASK_REG, UART0_PSELRXD_REG, UART0_PSELTXD_REG, UART0_PSELRTS_REG, UART0_PSELCTS_REG, UART0_BAUDRATE_REG, UART0_RXD_PTR_REG, UART0_RXD_MAXCNT_REG, UART0_RXD_AMOUNT_REG, UART0_RXD_LIST_REG, UART0_RXD_LIST_MAXCNT_REG, UART0_RXD_LIST_AMOUNT_REG, UART0_TXD_PTR_REG, UART0_TXD_MAXCNT_REG, UART0_TXD_AMOUNT_REG, UART0_TXD_LIST_REG, UART0_TXD_LIST_MAXCNT_REG, UART0_TXD_LIST_AMOUNT_REG, UART0_STATUS_REG, UART0_ERRORSRC_MASK, UART0_ENABLE_MASK, UART0_MATCH_MASK, UART0_MASK_MASK, UART0_PSELRXD_MASK, UART0_PSELTXD_MASK, UART0_PSELRTS_MASK, UART0_PSELCTS_MASK, UART0_BAUDRATE_MASK, UART0_RXD_PTR_MASK, UART0_RXD_MAXCNT_MASK, UART0_RXD_AMOUNT_MASK, UART0_RXD_LIST_MASK, UART0_RXD_LIST_MAXCNT_MASK, UART0_RXD_LIST_AMOUNT_MASK, UART0_TXD_PTR_MASK, UART0_TXD_MAXCNT_MASK, UART0_TXD_AMOUNT_MASK'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_bot(rag_pipeline, \"List UART regsiters for nrf52840 Microcontroller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1475a46b-1a95-4cbc-853e-f336bf7269ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asmita/.conda/envs/llm_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'UART0_TXD_LIST_MASK\\nUART0_TXD_LIST_MAXCNT_MASK\\nUART0_TXD_LIST_AMOUNT_MASK\\nUART0_STATUS_MASK\\nUART0_STATUS_MAXCNT\\nUART0_STATUS_AMOUNT\\nUART0_STATUS_LIST\\nUART0_STATUS_LIST_MAXCNT\\nUART0_STATUS_LIST_AMOUNT\\nUART0_ERRORSRC_MAXCNT\\nUART0_ERRORSRC_AMOUNT\\nUART0_ERRORSRC_LIST\\nUART0_ERRORSRC_LIST_MAXCNT\\nUART0_ERRORSRC_LIST_AMOUNT\\nUART0_ENABLE_MAXCNT\\nUART0_ENABLE_AMOUNT\\nUART0_ENABLE_LIST\\nUART0_ENABLE_LIST_MAXCNT\\nUART0_ENABLE_LIST_AMOUNT\\nUART0_MATCH_MAXCNT\\nUART0_MATCH_AMOUNT\\nUART0_MATCH_LIST\\nUART0_MATCH_LIST_MAXCNT\\nUART0_MATCH_LIST_AMOUNT\\nUART0_MASK_MAXCNT\\nUART0_MASK_AMOUNT\\nUART0_MASK_LIST\\nUART0_MASK_LIST_MAXCNT\\nUART0_MASK_LIST_AMOUNT\\nUART0_PSELRXD_MAXCNT\\nUART0_PSELRXD_AMOUNT\\nUART0_PSELRXD_LIST\\nUART0_PSELRXD_LIST_MAXCNT\\nUART0_PSELRXD_LIST_AMOUNT\\nUART0_PSELTXD_MAXCNT\\nUART0_PSELTXD_AMOUNT\\nUART0_PSELTXD_LIST\\nUART0_PSELTXD_LIST_MAXCNT\\nUART0_PSELTXD_LIST_AMOUNT\\nUART0_PSELRTS_MAXCNT\\nUART0_PSELRTS_AMOUNT\\nUART0'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_bot_new1(rag_pipeline, \"List UART regsiters for nrf52840 Microcontroller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce794f25-bbf9-4602-a4f9-de3aa8593cb3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ask_bot_manual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mask_bot_manual\u001b[49m(retriever_multi_vector_img, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList UART regsiters for nrf52840 Microcontroller\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ask_bot_manual' is not defined"
     ]
    }
   ],
   "source": [
    "ask_bot_manual(retriever_multi_vector_img, \"List UART regsiters for nrf52840 Microcontroller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afe7f234-d3b9-4374-905b-cd53d433fa40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asmita/.conda/envs/llm_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_bot_new1(rag_pipeline, \"what is base address of SPI peripheral  for nrf52840 Microcontroller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541cff8-b320-48b0-aa24-059cfb7f0144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8c301-8637-465f-8c4e-934c569eefe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8775e22-f445-41d8-8f7c-9da37b078c56",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ask_bot_manual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mask_bot_manual\u001b[49m(retriever_multi_vector_img, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaht is the base address of SPI peripheral  for nrf52840 Microcontroller\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ask_bot_manual' is not defined"
     ]
    }
   ],
   "source": [
    "ask_bot_manual(retriever_multi_vector_img, \"Waht is the base address of SPI peripheral  for nrf52840 Microcontroller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "18239808-fd94-46ee-b484-bb45151e3e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output : ### Instruction:\n",
      "Context:\n",
      "You are an expert on microcontrollers and can provide detailed information about their peripherals, registers, and fields.\n",
      "Answer the user question concisely based  on the context provided.\n",
      "ONLY output the direct answer as a word, number, address, or short phrase.\n",
      "Do NOT repeat the question or context. Do NOT give explanations or full sentences.\n",
      "\n",
      "\n",
      "\n",
      "Question: what is base address of SPI peripheral  for nrf52840 Microcontroller\n",
      "\n",
      "### Response:\n",
      "0x40003000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*********************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0x40003000'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_bot(rag_pipeline, \"what is base address of SPI peripheral  for nrf52840 Microcontroller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f7c63-8ad9-4c2f-9617-77b6fa5aec1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
