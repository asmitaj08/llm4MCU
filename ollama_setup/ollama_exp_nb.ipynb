{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ecfdce-d832-439f-a53e-f7058bc1068a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !/home/user/ollama/bin/ollama serve # running it via a seperate sbatch script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c42bc-d0b4-45d3-9df2-90f3385cd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/home/user/ollama/bin/ollama pull llama3.2:latest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee94475-2794-4875-b578-3adb28ca560f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !/home/user/ollama/bin/ollama pull deepseek-r1:14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b789b-5ee5-41de-934e-5f93141fa9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !/home/user/ollama/bin/ollama run llama3.2:latest # running it via a seperate sbatch script or directly in node's terminal or via python script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b67032-50e4-45ae-b833-9e0d6ff35d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE     PROCESSOR    UNTIL              \n",
      "deepseek-r1:14b    ea35dfe18182    11 GB    100% GPU     2 minutes from now    \n"
     ]
    }
   ],
   "source": [
    "!/home/user/ollama/bin/ollama ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a209e6b-0376-47d8-9255-3a094c741bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED       \n",
      "deepseek-r1:14b    ea35dfe18182    9.0 GB    24 seconds ago    \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    2 months ago      \n"
     ]
    }
   ],
   "source": [
    "!/home/user/ollama/bin/ollama ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23a485-7ad6-49e0-8feb-d0b66c63509c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23e37044-f129-4151-ae35-30f07e447dbf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight reaches Earth's atmosphere, it interacts with molecules and small particles in the air. Sunlight is composed of various colors, each corresponding to different wavelengths. Blue light has a shorter wavelength compared to other colors like red or orange. Shorter wavelengths are scattered more by the atmosphere, causing blue light to be dispersed in all directions. As a result, we see the sky as blue because our eyes receive scattered blue light from all over the sky.\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight reaches Earth's atmosphere, it interacts with molecules and small particles in the air. Sunlight is composed of various colors, each corresponding to different wavelengths. Blue light has a shorter wavelength compared to other colors like red or orange. Shorter wavelengths are scattered more by the atmosphere, causing blue light to be dispersed in all directions. As a result, we see the sky as blue because our eyes receive scattered blue light from all over the sky.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='deepseek-r1:14b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a460f-82ad-4060-a4b2-f1da72af165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?25l\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !/home/user/ollama/bin/ollama stop llama3.2:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4b881-d2b4-47a1-bcdf-11072e77e57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?25l\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!/home/user/ollama/bin/ollama stop deepseek-r1:14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79335dfc-0071-49dc-9953-88cebafeab5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"unstructured[all-docs]\" chromadb pydantic lxml tiktoken langchain langchain-community langchain_ollama langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a37d856-f5d9-478e-a786-c09893624bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Done!!\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "from tqdm import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "from langchain_ollama.llms import OllamaLLM #for standard\n",
    "from langchain_ollama.chat_models import ChatOllama # for chat-based models\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.prompts import PromptTemplate\n",
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import base64\n",
    "import time\n",
    "\n",
    "print(\"Imports Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16fa76b6-29b0-4759-ab9a-4766dead9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../data_ollama_deepseek_r1_14b/pdfs/nRF52840_PS_v1.11.pdf\"\n",
    "image_path = \"../data_ollama_deepseek_r1_14b/images_collect/images_nrf52820\"\n",
    "db_path = \"../data_ollama_deepseek_r1_14b/pdf_partitioning_result/chroma_dbs/chroma_db_nrf52820\"\n",
    "pickle_path = \"../data_ollama_deepseek_r1_14b/pdf_partitioning_result/pickle_files/nrf52820.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747eaf8-d9cb-41e0-b378-5cc0337ed1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad290282-ebd8-409d-a22d-3afa0526eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-r1:14b\" \n",
    "embedd_model = \"nomic-embedd-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ade5da1c-357b-4525-ba2d-5240b2a56e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroma_db(local_directory=db_path):\n",
    "    embeddings = OllamaEmbeddings(model=embedd_model)\n",
    "    return Chroma(persist_directory=local_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0e08c3-a601-46db-b801-0cc513899f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_elements(raw_pdf_elements):\n",
    "    text_elements = []\n",
    "    table_elements = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if 'CompositeElement' in str(type(element)):\n",
    "            text_elements.append(str(element))\n",
    "        elif 'Table' in str(type(element)):\n",
    "            table_elements.append(str(element))\n",
    "    return text_elements, table_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f2535d-ad8c-4996-b176-b0ba158cac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=model_name,temperature=0, num_predict=1024) #kept parameters same as in case of openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c89e99a1-64b4-44d8-8d97-fbf949f33744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well-optimized for retrieval. \\\n",
    "    Don't use Markdown, just plain text output. Table \\\n",
    "    or text: {element} \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
    "\n",
    "    return text_summaries, table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ffa2bb-84a5-4585-8398-5f7bca386e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5db53b3-a71a-41ca-8329-ce136b66b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    msg = model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab8ef51f-e58d-495f-ba27-5fe5cf40e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Include all the values in each image, including extracting all the text. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bdb625f-ab9d-4baf-a064-c3fd44417d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128140ef-97ed-4b4b-8ce5-afed4f6bca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Chroma database...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(db_path) and os.path.exists(pickle_path):\n",
    "    print(\"Loading existing Chroma database...\")\n",
    "    vectorstore = load_chroma_db()\n",
    "\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "\n",
    "    # Access the variables\n",
    "    texts = loaded_data['texts']\n",
    "    tables = loaded_data['tables']\n",
    "    text_summaries = loaded_data['text_summaries']\n",
    "    table_summaries = loaded_data['table_summaries']\n",
    "    img_base64_list = loaded_data['img_base64_list']\n",
    "    image_summaries = loaded_data['image_summaries']\n",
    "\n",
    "else:\n",
    "    print(\"Creating new Chroma database...\")\n",
    "    # Store embeddings in Chroma\n",
    "    start_time = time.time()\n",
    "    pdf_elements = partition_pdf(\n",
    "        pdf_path,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        extract_image_block_types=['Table', 'Image'],\n",
    "        extract_image_block_output_dir=image_path,\n",
    "        max_characters=3000,\n",
    "        new_after_n_chars=2800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=image_path\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"pdf partition done!! Time : {end_time-start_time}\")\n",
    "    \n",
    "    # extract tables and texts\n",
    "    start_time = time.time()\n",
    "    texts, tables = categorize_elements(pdf_elements)\n",
    "    end_time = time.time()\n",
    "    print(f\"categorize elements done!! Time : {end_time - start_time}\")\n",
    "    \n",
    "    # Get text & table summaries\n",
    "    start_time = time.time()\n",
    "    text_summaries, table_summaries = generate_text_summaries(texts[0:19], tables, summarize_texts=True)\n",
    "    end_time = time.time()\n",
    "    print(f\"generate text summaries done!! Time : {end_time - start_time}\")\n",
    "\n",
    "    # Image summaries\n",
    "    start_time = time.time()\n",
    "    img_base64_list, image_summaries = generate_img_summaries(image_path)\n",
    "    end_time = time.time()\n",
    "    print(f\"generate img summaries done!! Time : {end_time - start_time}\")\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'texts': texts,\n",
    "            'tables': tables,\n",
    "            'text_summaries': text_summaries,\n",
    "            'table_summaries': table_summaries,\n",
    "            'img_base64_list': img_base64_list,\n",
    "            'image_summaries': image_summaries\n",
    "        }, f)\n",
    "    print(\"Dumped pickle file\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"mm_rag\",\n",
    "        embedding_function =OllamaEmbeddings(model=embedd_model),\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"vectorstore done!! Time : {end_time - start_time}\")\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55ea80-5aad-4f52-a7cc-5d6cc9bc3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 : After pdf partitioning & embedded vector creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0990b-fcaf-42f1-ba16-228a57f547e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c5342-a084-477a-9b25-70163d72f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    if len(b64_images) > 0:\n",
    "        return {\"images\": b64_images[:1], \"texts\": []}\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c8fcd-2ca1-4b63-844a-61f74882389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5dfb0-3a5b-4f0f-ab76-6c5869a564d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an AI scientist tasking with providing factual answers from a datasheet of a System-on-Chip (SoC) \\n\"\n",
    "            \"Use this information to provide answers related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model  # MM_LLM\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe6cbd-30aa-4ddc-9e30-17344203390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a03908-f4ac-4d1d-9b8e-7acbbe347d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bot(query):\n",
    "    return chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60113bfe-929f-4bf4-8104-055326fef5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_bot(\"What is the base address of UART\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
