{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Path to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"unstructured[all-docs]\" chromadb pydantic lxml tiktoken langchain langchain-community langchain-openai langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from sentence-transformers) (0.24.6)\n",
      "Requirement already satisfied: Pillow in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/python3_10/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/python/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "# import pytesseract\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.prompts import PromptTemplate\n",
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './nRF52840_PS_v1.8.pdf'\n",
    "uart_pdf_path='./uart.pdf'\n",
    "image_path = \"./images/\"\n",
    "db_path = \"./chroma_langchain_db\"\n",
    "pickle_path = \"./stored_data.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest PDF \n",
    "\n",
    "Split the pdf into text chunks, save any table and images as images and store them into a separate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroma_db(local_directory=db_path):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    return Chroma(persist_directory=local_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_elements(raw_pdf_elements):\n",
    "    text_elements = []\n",
    "    table_elements = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if 'CompositeElement' in str(type(element)):\n",
    "            text_elements.append(str(element))\n",
    "        elif 'Table' in str(type(element)):\n",
    "            table_elements.append(str(element))\n",
    "    return text_elements, table_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate summaries for the text blocks and table/images. \n",
    "\n",
    "These descriptions help to match a query better, so that we don't have to deal with the spaces and formatting from the direct extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well-optimized for retrieval. \\\n",
    "    Don't use Markdown, just plain text output. Table \\\n",
    "    or text: {element} \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
    "\n",
    "    return text_summaries, table_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "# encode image\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    msg = model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Include all the values in each image, including extracting all the text. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vector database to store summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "    \n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(db_path) and os.path.exists(pickle_path):\n",
    "#     print(\"Loading existing Chroma database...\")\n",
    "#     vectorstore = load_chroma_db()\n",
    "    \n",
    "#     with open(pickle_path, 'rb') as f:\n",
    "#         loaded_data = pickle.load(f)\n",
    "\n",
    "#     # Access the variables\n",
    "#     texts = loaded_data['texts']\n",
    "#     tables = loaded_data['tables']\n",
    "#     text_summaries = loaded_data['text_summaries']\n",
    "#     table_summaries = loaded_data['table_summaries']\n",
    "#     img_base64_list = loaded_data['img_base64_list']\n",
    "#     image_summaries = loaded_data['image_summaries']\n",
    "\n",
    "# else:\n",
    "#     print(\"Creating new Chroma database...\")\n",
    "#     # Store embeddings in Chroma\n",
    "    \n",
    "#     pdf_elements = partition_pdf(\n",
    "#         pdf_path,\n",
    "#         chunking_strategy=\"by_title\",\n",
    "#         extract_images_in_pdf=True,\n",
    "#         infer_table_structure=True,\n",
    "#         extract_image_block_types=['Table', 'Image'],\n",
    "#         extract_image_block_output_dir='./images',\n",
    "#         max_characters=3000,\n",
    "#         new_after_n_chars=2800,\n",
    "#         combine_text_under_n_chars=2000,\n",
    "#         image_output_dir_path=image_path\n",
    "#     )\n",
    "    \n",
    "#     # extract tables and texts\n",
    "#     texts, tables = categorize_elements(pdf_elements)\n",
    "    \n",
    "#     # Get text & table summaries\n",
    "#     text_summaries, table_summaries = generate_text_summaries(texts[0:19], tables, summarize_texts=True)\n",
    "    \n",
    "#     # Image summaries\n",
    "#     img_base64_list, image_summaries = generate_img_summaries(\"./images\")\n",
    "    \n",
    "#     with open(pickle_path, 'wb') as f:\n",
    "#         pickle.dump({\n",
    "#             'texts': texts,\n",
    "#             'tables': tables,\n",
    "#             'text_summaries': text_summaries,\n",
    "#             'table_summaries': table_summaries,\n",
    "#             'img_base64_list': img_base64_list,\n",
    "#             'image_summaries': image_summaries\n",
    "#         }, f)\n",
    "    \n",
    "#     vectorstore = Chroma(\n",
    "#         collection_name=\"mm_rag\",\n",
    "#         embedding_function = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\")),\n",
    "#         persist_directory=\"./chroma_langchain_db\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "# retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "#     vectorstore,\n",
    "#     text_summaries,\n",
    "#     texts,\n",
    "#     table_summaries,\n",
    "#     tables,\n",
    "#     image_summaries,\n",
    "#     img_base64_list,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    if len(b64_images) > 0:\n",
    "        return {\"images\": b64_images[:1], \"texts\": []}\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare RAG pipeline  \n",
    "user can ask question and the query will search for relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an AI scientist tasking with providing factual answers from a datasheet of a System-on-Chip (SoC) \\n\"\n",
    "            \"Use this information to provide answers related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model  # MM_LLM\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bot(chain_multimodal_rag, query):\n",
    "    # docs = retriever_multi_vector_img.get_relevant_documents(query, limit=10)\n",
    "    # print(split_image_text_types(docs))\n",
    "    return chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match chroma dbs, pickle files \n",
    "# (dataset key) -> chroma db path, pickle path, evaluate dataset folder path \n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "chroma_dir = \"./pdf_partitioning_result/chroma_dbs\"\n",
    "pickle_dir = \"./pdf_partitioning_result/pickle_files\"\n",
    "evaluate_dir = \"./evaluation_mcu_svd_dataset\"\n",
    "\n",
    "dataset_dict = defaultdict(list)\n",
    "\n",
    "def extract_key(filename):\n",
    "    return filename.split('db_')[-1] if '_' in filename else filename.split('.pkl')[0]\n",
    "\n",
    "def qa_key(filename):\n",
    "    return filename.split('datasets_')[-1]\n",
    "\n",
    "# Aggregate file paths \n",
    "for root, dirs, files in os.walk(chroma_dir):\n",
    "    for dirname in dirs:\n",
    "        key = extract_key(dirname)\n",
    "        # print(key)\n",
    "        # print('lpc1102_04.pkl')\n",
    "        dataset_dict[key].append(os.path.join(root, dirname))\n",
    "\n",
    "for root, dirs, files in os.walk(pickle_dir):\n",
    "    for file in files:\n",
    "        # print(file.split('.pkl')[0])\n",
    "        key = extract_key(file)\n",
    "        if key == 'lpc1102_04.pkl':\n",
    "            key = 'lpc1102_04'\n",
    "        dataset_dict[key].append(os.path.join(root, file))\n",
    "\n",
    "for root, dirs, files in os.walk(evaluate_dir):\n",
    "    for dirname in dirs:\n",
    "        key = qa_key(dirname)\n",
    "        # print(key)\n",
    "        if key == 'qn908xc':\n",
    "            key = 'QN9080x'\n",
    "        elif key == 'stm32f100xx':\n",
    "            key = 'stm32f100'\n",
    "        dataset_dict[key].append(os.path.join(root, dirname))\n",
    "        \n",
    "        \n",
    "dataset_dict = dict(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rag(chroma_path, pickle_path):\n",
    "    # if os.path.exists(db_path) and os.path.exists(pickle_path):\n",
    "    print(\"Loading existing Chroma database...\")\n",
    "    vectorstore = load_chroma_db()\n",
    "    \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "\n",
    "    # Access the variables\n",
    "    texts = loaded_data['texts']\n",
    "    tables = loaded_data['tables']\n",
    "    text_summaries = loaded_data['text_summaries']\n",
    "    table_summaries = loaded_data['table_summaries']\n",
    "    img_base64_list = loaded_data['img_base64_list']\n",
    "    image_summaries = loaded_data['image_summaries']\n",
    "\n",
    "    retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "        vectorstore,\n",
    "        text_summaries,\n",
    "        texts,\n",
    "        table_summaries,\n",
    "        tables,\n",
    "        image_summaries,\n",
    "        img_base64_list,\n",
    "    )\n",
    "    chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)\n",
    "    return chain_multimodal_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load your finetuned LLM model here\n",
    "# For example:\n",
    "# model = YourLLMModel.load_from_checkpoint('path_to_checkpoint')\n",
    "\n",
    "# Load the pre-trained Sentence Transformer model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def compute_similarity(model_output: str, ground_truth: str) -> tuple:\n",
    "    # Same as before\n",
    "    embeddings = embedding_model.encode([model_output, ground_truth])\n",
    "    cos_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "import openai\n",
    "random.seed(42)\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "\n",
    "def evaluate(name=\"\", rag = None, json_q_a_file_path=\"./nrf52840.json\"):\n",
    "    # Same as before\n",
    "    # with open(json_q_a_file_path, 'r') as file:\n",
    "    #     q_a = json.load(file)\n",
    "    with open(json_q_a_file_path, 'r', encoding='utf-8') as file:\n",
    "        q_a = [json.loads(line) for line in file]\n",
    "        \n",
    "    # Randomly select 3 examples for few-shot context\n",
    "    few_shot_examples = random.sample(q_a, 3)\n",
    "\n",
    "    # Create the few-shot context\n",
    "    few_shot_context = \"\\n\".join(\n",
    "        f\"Example Question: {example['messages'][0]['content']} {example['messages'][1]['content']}\\n\"\n",
    "        f\"Example Answer: {example['messages'][2]['content']}\"\n",
    "        for example in few_shot_examples\n",
    "    ) + \"\\n\\n\"\n",
    "\n",
    "    # Create a subset excluding the few-shot examples\n",
    "    remaining_q_a = [example for example in q_a if example not in few_shot_examples]\n",
    "\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    def process_q_a(q_a_pair):\n",
    "        question = q_a_pair[\"messages\"][0][\"content\"] + \" \" + q_a_pair[\"messages\"][1][\"content\"]\n",
    "        ground_truth = q_a_pair[\"messages\"][2][\"content\"]\n",
    "        reg_model_output = ask_bot(rag, question)\n",
    "        few_shot_model_output = ask_bot(rag, few_shot_context + \" \" + question)\n",
    "        \n",
    "        reg_cos_sim = compute_similarity(reg_model_output, ground_truth)\n",
    "        few_shot_cos_sim = compute_similarity(few_shot_model_output, ground_truth)\n",
    "        \n",
    "\n",
    "        ft_reg = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-2024-08-06:ucd-aseec:svd-finetune:AUNjCF3m',\n",
    "            messages=[q_a_pair[\"messages\"][0], q_a_pair[\"messages\"][1]],\n",
    "            max_tokens=1024,\n",
    "            temperature=0\n",
    "        )\n",
    "        ft_reg_model_output = ft_reg.choices[0].message.content\n",
    "        \n",
    "        ft_few_shot = client.chat.completions.create(\n",
    "            model='ft:gpt-4o-2024-08-06:ucd-aseec:svd-finetune:AUNjCF3m',\n",
    "            messages=[{\"role\": \"system\", \"content\":few_shot_context}, q_a_pair[\"messages\"][0], q_a_pair[\"messages\"][1]],\n",
    "            max_tokens=1024,\n",
    "            temperature=0\n",
    "        )    \n",
    "        \n",
    "        ft_few_shot_model_output = ft_few_shot.choices[0].message.content\n",
    "        \n",
    "        ft_reg_cos_sim = compute_similarity(ft_reg_model_output, ground_truth)\n",
    "        ft_few_shot_cos_sim = compute_similarity(ft_few_shot_model_output, ground_truth)\n",
    "        \n",
    "        return {\n",
    "            'datasheet': name,\n",
    "            'question': question,\n",
    "            'ground_truth': ground_truth,\n",
    "            'baseline model_output': reg_model_output,\n",
    "            'few shot model_output': few_shot_model_output,\n",
    "            'baseline cosine_similarity': reg_cos_sim,\n",
    "            'ft cosine_similarity': ft_reg_cos_sim,\n",
    "            'few_shot cosine_similarity': few_shot_cos_sim,\n",
    "            'ft few_shot cosine_similarity': ft_few_shot_cos_sim\n",
    "            \n",
    "        }\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_q_a, q_a_pair) for q_a_pair in remaining_q_a]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            scores.append(future.result())\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./pdf_partitioning_result/chroma_dbs/chroma_db_nrf52840',\n",
       " './pdf_partitioning_result/pickle_files/nrf52840.pkl',\n",
       " './evaluation_mcu_svd_dataset/datasets_nrf52840']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['nrf52840']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasheet_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrf52840\n",
      "Loading existing Chroma database...\n",
      "Finished Creating RAG pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2368/21079 [14:56<1:34:12,  3.31it/s]"
     ]
    }
   ],
   "source": [
    "# for each dataset pair, load dataset based on key (chroma, pickle, eval_db (open db to get main.jsonl)) \n",
    "# call load for every chroma, pickle\n",
    "# call evaluate to get scores for every question in key\n",
    "\n",
    "# for key, file_paths in dataset_dict.items():\n",
    "for key, file_paths in [('nrf52840', ['./pdf_partitioning_result/chroma_dbs/chroma_db_nrf52840',\n",
    " './pdf_partitioning_result/pickle_files/nrf52840.pkl',\n",
    " './evaluation_mcu_svd_dataset/datasets_nrf52840'])]:\n",
    "    print(key)\n",
    "    chroma_db_path, pickle_path, eval_q_a_json_path = file_paths\n",
    "    rag_pipeline = init_rag(chroma_db_path, pickle_path)\n",
    "    print(\"Finished Creating RAG pipeline\")\n",
    "    \n",
    "    main_data_path = os.path.join(eval_q_a_json_path, \"main_data.jsonl\")\n",
    "    all_scores = evaluate(key, rag_pipeline, main_data_path)    \n",
    "    print(\"Finished Evaluation\")\n",
    "    \n",
    "    all_datasheet_scores.extend(all_scores)\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
